{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practica 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 1: Particionado\n",
    "Análisis de las dos estrategias de particionado propuestas: simple, y cruzada, para los conjuntos propuestos: german y tic-tac-toe. El análisis consiste en una descripción de los índices de train y test devueltos por cada uno de los métodos de particionado, junto con un comentario sobre las ventajas/desventajas de cada uno de ellos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple validation assert: 0.3\n",
      "Cross validation assert: [0.732, 0.748, 0.752, 0.748]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from datos import *\n",
    "from EstrategiaParticionado import *\n",
    "#from Clasificador import *\n",
    "\n",
    "#Este hay que cambiar a funciones de clasificador\n",
    "from naive_bayes_functions import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Validación simple__: Este metodo de particionado divide los datos en 2 partes dada la proporción de datos test y train. Antes de la división los ids de las filas están mezcladas de manera aleatoria para asegurarse que los datos en train and test no esten en el mismo orden que  en el conjunto de datos original (por si están ordenados por clases o algún atributo.\n",
    "\n",
    "__Ventajas__: Implementación simple y rápida\n",
    "\n",
    "__Desventajas__: Si el conjunto de datos es pequeño quedan pocos datos para la validación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test:  [0, 2, 3, 4, 5, 6, 10, 14, 16, 24, 26, 27, 29, 34, 37, 39, 48, 52, 55, 56, 60, 62, 66, 68, 71, 72, 78, 80, 81, 84, 95, 96, 97, 99, 100, 103, 104, 106, 107, 109, 117, 119, 120, 121, 128, 130, 131, 132, 135, 136, 137, 138, 141, 147, 149, 151, 154, 160, 166, 167, 171, 185, 192, 197, 204, 210, 215, 223, 228, 232, 236, 237, 238, 240, 241, 247, 251, 252, 253, 257, 258, 261, 263, 271, 274, 277, 279, 281, 284, 289, 292, 294, 296, 300, 306, 311, 312, 313, 321, 322, 323, 324, 327, 338, 339, 344, 356, 358, 360, 363, 364, 367, 368, 374, 378, 387, 390, 393, 395, 396, 399, 403, 410, 411, 413, 421, 422, 428, 432, 437, 439, 440, 442, 445, 454, 463, 465, 466, 469, 473, 480, 482, 488, 490, 491, 493, 504, 506, 507, 508, 509, 515, 516, 517, 521, 522, 526, 536, 538, 541, 548, 550, 552, 553, 557, 566, 569, 571, 574, 576, 581, 582, 589, 590, 591, 592, 593, 599, 616, 617, 618, 623, 626, 629, 633, 634, 636, 638, 641, 646, 651, 655, 656, 658, 661, 666, 667, 670, 675, 676, 684, 687, 690, 694, 696, 700, 704, 705, 708, 712, 714, 719, 720, 723, 729, 732, 733, 736, 737, 738, 741, 742, 745, 746, 747, 748, 755, 756, 761, 765, 766, 768, 771, 772, 774, 778, 780, 783, 784, 791, 792, 793, 794, 799, 800, 804, 805, 806, 811, 816, 817, 818, 820, 822, 824, 827, 839, 841, 843, 847, 849, 851, 853, 859, 879, 887, 889, 895, 896, 897, 899, 901, 904, 907, 910, 915, 917, 923, 927, 931, 934, 939, 941, 945, 948, 951, 954]\n"
     ]
    }
   ],
   "source": [
    "dataset=Datos('./ConjuntosDatos/tic-tac-toe.data') \n",
    "\n",
    "rows_number=dataset.datos.shape[0]\n",
    "\n",
    "#Simple validation\n",
    "test_proportion=0.3\n",
    "line_ids=validacion_simple(rows_number,test_proportion)\n",
    "line_ids_test=line_ids['Test']\n",
    "line_ids_train=line_ids['Train']\n",
    "\n",
    "print(\"Test: \", line_ids_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para extraer los datos se utiliza el metodo extraeDatos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['x' 'x' 'x' ... 'o' 'x' 'positive']\n",
      " ['x' 'x' 'x' ... 'b' 'o' 'positive']\n",
      " ['x' 'x' 'x' ... 'o' 'b' 'positive']\n",
      " ...\n",
      " ['x' 'o' 'x' ... 'x' 'o' 'negative']\n",
      " ['o' 'x' 'x' ... 'x' 'x' 'negative']\n",
      " ['o' 'x' 'o' ... 'o' 'x' 'negative']]\n"
     ]
    }
   ],
   "source": [
    "train=dataset.extraeDatos(line_ids_train)\n",
    "test=dataset.extraeDatos(line_ids_test)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Validación cruzada__: En este tipo de validación el conjunto de datos de divide de manera aleatoria en N grupos, y uno de los grupos se utiliza para test y otros N-1 para train. De esta manera generamos N diferentes subconjuntos de train/test.\n",
    "![alt text](K-fold_cross_validation.jpg \"K-Fold Cross Validation\")\n",
    "\n",
    "__Ventajas__:Se utiliza el conjunto completo de datos para entrenamiento y para la validación y se puede generar más metricas para ver si el modelo el robusto.\n",
    "\n",
    "__Desventajas__: El algoritmo hay que repetir N veces, lo que conlleva mayor coste computacional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ids con segundo grupo asignado como test y train el resto:\n",
      "[1, 2, 3, 4, 13, 18, 31, 34, 39, 41, 48, 50, 57, 61, 65, 66, 69, 72, 74, 75, 89, 91, 93, 94, 98, 101, 105, 109, 112, 120, 127, 128, 129, 130, 138, 139, 158, 161, 169, 173, 175, 178, 182, 184, 185, 191, 193, 195, 199, 202, 204, 206, 210, 214, 220, 221, 223, 226, 236, 238, 239, 241, 243, 245, 246, 248, 251, 256, 259, 265, 278, 280, 286, 287, 291, 292, 295, 296, 301, 304, 310, 314, 326, 332, 335, 336, 338, 343, 350, 355, 356, 358, 360, 364, 365, 367, 368, 370, 380, 389, 390, 395, 397, 398, 408, 409, 412, 414, 416, 422, 428, 430, 435, 437, 441, 442, 447, 449, 450, 453, 455, 467, 470, 472, 473, 474, 476, 480, 482, 484, 489, 497, 504, 507, 512, 513, 514, 517, 523, 535, 536, 547, 552, 553, 556, 568, 571, 572, 581, 593, 594, 595, 602, 604, 606, 617, 621, 623, 630, 633, 636, 638, 639, 645, 646, 647, 652, 662, 666, 668, 675, 676, 683, 688, 694, 697, 698, 702, 707, 708, 713, 726, 729, 731, 737, 738, 742, 745, 748, 754, 756, 759, 761, 766, 767, 769, 776, 777, 780, 783, 785, 799, 801, 808, 813, 814, 816, 819, 821, 823, 835, 837, 840, 841, 848, 856, 862, 872, 874, 875, 879, 882, 885, 886, 887, 889, 890, 904, 906, 908, 918, 922, 923, 924, 926, 927, 936, 943, 948, 951]\n"
     ]
    }
   ],
   "source": [
    "partitions=4\n",
    "line_ids=validacion_cruzada(rows_number,partitions)\n",
    "print(\"Test ids con segundo grupo asignado como test y train el resto:\")\n",
    "print(line_ids[1]['Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 2: Naive-Bayes\n",
    "Tabla con los resultados de la ejecución para los conjuntos de datos analizados (tic-tac-toe y german). Considerar los dos tipos de particionado.\n",
    "Los resultados se refieren a las tasas de error/acierto y deben incluirse tanto con la corrección de Laplace como sin ella. Se debe incluir tanto el promedio de error para las diferentes particiones como su desviación típica. Es importante mostrar todos los resultados agrupados en una tabla para facilitar su evaluación.\n",
    "Breve análisis de los resultados anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tic tac toe error, laplace =True 0.344\n",
      "Tic tac toe error, laplace =False 0.344\n",
      "German error, laplace=True 0.272\n",
      "German error, laplace=False 0.272\n"
     ]
    }
   ],
   "source": [
    "tictac=Datos('./ConjuntosDatos/tic-tac-toe.data') \n",
    "\n",
    "rows_tictac=tictac.datos.shape[0]\n",
    "\n",
    "test_proportion=0.3\n",
    "\n",
    "line_ids_tic=validacion_simple(rows_tictac,test_proportion)\n",
    "line_ids_test_tic=line_ids_tic['Test']\n",
    "line_ids_train_tic=line_ids_tic['Train']\n",
    "\n",
    "train_tic=tictac.extraeDatos(line_ids_train_tic)\n",
    "test_tic=tictac.extraeDatos(line_ids_test_tic)\n",
    "\n",
    "print(\"Tic tac toe error, laplace =True\", validacion(test_tic,tictac,train_tic, True))\n",
    "print(\"Tic tac toe error, laplace =False\", validacion(test_tic,tictac,train_tic, False))\n",
    "\n",
    "\n",
    "german=Datos('./ConjuntosDatos/german.data') \n",
    "\n",
    "rows_german=german.datos.shape[0]\n",
    "\n",
    "test_proportion=0.3\n",
    "#Aqui tienen que estar las funciones de validacion de clasificador\n",
    "line_ids_german=validacion_simple(rows_german,test_proportion)\n",
    "line_ids_test_german=line_ids_german['Test']\n",
    "line_ids_train_german=line_ids_german['Train']\n",
    "\n",
    "train_german=german.extraeDatos(line_ids_train_german)\n",
    "test_german=german.extraeDatos(line_ids_test_german)\n",
    "\n",
    "\n",
    "print(\"German error, laplace=True\", validacion(test_german,german,train_german, True))\n",
    "print(\"German error, laplace=False\", validacion(test_german,german,train_german, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tic-tac-toe\n",
      "Without laplace [0.316, 0.284, 0.256, 0.26] Mean:  0.279 Std: 0.023895606290697036\n",
      "With Laplace [0.316, 0.284, 0.256, 0.26] Mean:  0.279 Std: 0.023895606290697036\n",
      "German\n",
      "Without laplace [0.284, 0.24, 0.244, 0.26] Mean:  0.257 Std: 0.01729161646579058\n",
      "With Laplace [0.284, 0.24, 0.244, 0.26] Mean:  0.257 Std: 0.01729161646579058\n"
     ]
    }
   ],
   "source": [
    "##Cruzada\n",
    "partitions=4\n",
    "#Tic tac\n",
    "line_ids_tic=validacion_cruzada(rows_tictac,partitions)\n",
    "assert_cross_tictac=[]\n",
    "assert_cross_tictac_laplace=[]\n",
    "for i in range(partitions):\n",
    "    line_ids_test=line_ids_tic[i]['Test']\n",
    "    line_ids_train=line_ids_tic[i]['Train']\n",
    "    train=tictac.extraeDatos(line_ids_train)\n",
    "    test=tictac.extraeDatos(line_ids_test)\n",
    "    assert_cross_tictac.append(validacion(test,tictac,train, False))\n",
    "    assert_cross_tictac_laplace.append(validacion(test,tictac,train, True))\n",
    "print(\"Tic-tac-toe\")\n",
    "print(\"Without laplace\", assert_cross_tictac, \"Mean: \", np.mean(assert_cross_tictac), \"Std:\", np.std(assert_cross_tictac))\n",
    "print(\"With Laplace\", assert_cross_tictac_laplace, \"Mean: \", np.mean(assert_cross_tictac_laplace), \"Std:\", np.std(assert_cross_tictac_laplace))\n",
    "\n",
    "\n",
    "\n",
    "line_ids_german=validacion_cruzada(rows_german,partitions)\n",
    "assert_cross_german=[]\n",
    "assert_cross_german_laplace=[]\n",
    "for i in range(partitions):\n",
    "    line_ids_test=line_ids_german[i]['Test']\n",
    "    line_ids_train=line_ids_german[i]['Train']\n",
    "    train=german.extraeDatos(line_ids_train)\n",
    "    test=german.extraeDatos(line_ids_test)\n",
    "    assert_cross_german.append(validacion(test,german,train, False))\n",
    "    assert_cross_german_laplace.append(validacion(test,german,train, True))\n",
    "print(\"German\")\n",
    "print(\"Without laplace\", assert_cross_german, \"Mean: \", np.mean(assert_cross_german), \"Std:\", np.std(assert_cross_german))\n",
    "print(\"With Laplace\", assert_cross_german_laplace, \"Mean: \", np.mean(assert_cross_german_laplace), \"Std:\", np.std(assert_cross_german_laplace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Error con validación simple__\n",
    "\n",
    "|Dataset|tic-tac-toe|german|\n",
    "|-------|-----------|------|\n",
    "|Error (Laplace)| 0.344| 0.272|\n",
    "|Error (Sin Laplace)| 0.344| 0.272|\n",
    "\n",
    "__Error con validación cruzada__\n",
    "\n",
    "|Dataset|tic-tac-toe|german|\n",
    "|-------|-----------|------|\n",
    "|Mean Error| 0.278 | 0.259|\n",
    "|Std| 0.02| 0.0134|\n",
    "\n",
    "\n",
    "El error aplicando y no aplicando Laplace para estos conjuntos de datos es igual, no influye mucho a la precisión de clasificador. \n",
    "El error de validación cruzada es más bajo porque hemos utilzado más datos para entrenamiento en cada de sus iteraciones (75% vs 70% en validación simple)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 3: Scikit-Learn\n",
    "Incluir los mismos resultados que en el apartado 2 pero usando los métodos del paquete scikit-learn. Comparar y analizar los resultados.\n",
    "\n",
    "__Sckit-Learn Gaussian Bayes__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error validación simple:\n",
      "Tic tac toe: 0.28222996515679444\n",
      "German: 0.25\n"
     ]
    }
   ],
   "source": [
    "#Tic-tac-toe\n",
    "\n",
    "def error_gaussian_bayes(train, test):\n",
    "    #Preparando los datos conviertiendo datos categoricos a numericos\n",
    "    le = preprocessing.LabelEncoder()\n",
    "\n",
    "    train_a=np.empty([train.shape[0], train.shape[1]])\n",
    "    test_a=np.empty([test.shape[0], test.shape[1]])\n",
    "    for i in range(train.shape[1]):\n",
    "        train_column=train[:,i]\n",
    "        train_column=le.fit_transform(train_column)\n",
    "        train_a[:,i]=np.transpose(train_column)\n",
    "    for i in range(test.shape[1]):    \n",
    "        test_column=test[:,i]\n",
    "        test_column=le.fit_transform(test_column)\n",
    "        test_a[:,i]=np.transpose(test_column)\n",
    "\n",
    "\n",
    "    input_train=[]\n",
    "    for row in range(train_a.shape[0]):\n",
    "        input_train.append(train_a[row,0:-1])\n",
    "    classes_encoded=train_a[:,-1]\n",
    "\n",
    "    validation=[]\n",
    "    real_class=[]\n",
    "    for row in range(test_a.shape[0]):\n",
    "        validation.append(test_a[row,0:-1])\n",
    "        real_class.append(test_a[row,-1])\n",
    "\n",
    "\n",
    "\n",
    "    clf = GaussianNB()\n",
    "\n",
    "    #clf = GaussianNB()\n",
    "    clf.fit(input_train, classes_encoded)\n",
    "\n",
    "    error=0\n",
    "    for r in range(len(validation)):\n",
    "        predicted=clf.predict([validation[r]])\n",
    "        if predicted!=real_class[r]:\n",
    "            error+=1\n",
    "    error=error/len(validation)\n",
    "    #print(\"Error validación simple:\", error)\n",
    "    #print(\"Fit score:\", clf.score(input_array, classes_encoded))\n",
    "    return(error)\n",
    "\n",
    "print(\"Error validación simple:\")\n",
    "print(\"Tic tac toe:\", error_gaussian_bayes(train_tic, test_tic))\n",
    "print(\"German:\", error_gaussian_bayes(train_german, test_german))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error validación cruzada:\n",
      "Tic-tac-toe [0.2916666666666667, 0.275, 0.24686192468619247, 0.30962343096234307] Mean:  0.28078800557880057 Std: 0.023099280277892393\n",
      "German [0.292, 0.268, 0.232, 0.276] Mean:  0.267 Std: 0.021977260975835904\n"
     ]
    }
   ],
   "source": [
    "##Cruzada\n",
    "partitions=4\n",
    "#Tic tac\n",
    "#line_ids_tic=validacion_cruzada(rows_tictac,partitions)\n",
    "#Utilizamos las mismas particiones que en apartado 2\n",
    "assert_cross_tictac=[]\n",
    "for i in range(partitions):\n",
    "    line_ids_test=line_ids_tic[i]['Test']\n",
    "    line_ids_train=line_ids_tic[i]['Train']\n",
    "    train=tictac.extraeDatos(line_ids_train)\n",
    "    test=tictac.extraeDatos(line_ids_test)\n",
    "    assert_cross_tictac.append(error_gaussian_bayes(train, test))\n",
    "print(\"Error validación cruzada:\")\n",
    "\n",
    "print(\"Tic-tac-toe\", assert_cross_tictac, \"Mean: \", np.mean(assert_cross_tictac), \"Std:\", np.std(assert_cross_tictac))\n",
    "\n",
    "#line_ids_german=validacion_cruzada(rows_german,partitions)\n",
    "assert_cross_german=[]\n",
    "for i in range(partitions):\n",
    "    line_ids_test=line_ids_german[i]['Test']\n",
    "    line_ids_train=line_ids_german[i]['Train']\n",
    "    train=german.extraeDatos(line_ids_train)\n",
    "    test=german.extraeDatos(line_ids_test)\n",
    "    assert_cross_german.append(error_gaussian_bayes(train, test))\n",
    "\n",
    "print(\"German\", assert_cross_german, \"Mean: \", np.mean(assert_cross_german), \"Std:\", np.std(assert_cross_german))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Multinomial Naive Bayes__:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apartado 4: Evaluación de hipótesis mediante Análisis ROC\n",
    "Matriz de confusión y diagramas del clasificador en el espacio ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "Tic tac toe:\n",
      "[[161.  58.]\n",
      " [ 28.  40.]]\n",
      "TPR: 0.85 FNR:  0.15 FPR:  0.59 TNR:  0.41\n",
      "German:\n",
      "[[188.  37.]\n",
      " [ 28.  34.]]\n",
      "TPR: 0.87 FNR:  0.13 FPR:  0.52 TNR:  0.48\n"
     ]
    }
   ],
   "source": [
    "#positive and negative are the classes names that need to be compared\n",
    "def confusion_mx(dataset, train, test, laplace, positive, negative):\n",
    "    confusion_matrix=np.zeros([2,2])\n",
    "    for r in range(test_tic.shape[0]):\n",
    "        predicted=naive_bayes(dataset, train, test[r][0:-1], laplace)\n",
    "        real=test[r][-1]\n",
    "        #print(\"Predicted \", predicted, \"Real \", classes_encoded[r])\n",
    "        if predicted==positive:\n",
    "            if real==positive:\n",
    "                confusion_matrix[0,0]+=1\n",
    "            else: \n",
    "                confusion_matrix[0,1]+=1\n",
    "        else:\n",
    "            if real==negative:\n",
    "                confusion_matrix[1,1]+=1\n",
    "            else: \n",
    "                confusion_matrix[1,0]+=1\n",
    "\n",
    "    return(confusion_matrix)\n",
    "\n",
    "confusion_matrix_tic=confusion_mx(tictac, train_tic, test_tic, True, \"positive\", \"negative\")\n",
    "print(\"Confusion matrix\")\n",
    "print(\"Tic tac toe:\")\n",
    "print(confusion_matrix_tic)\n",
    "tp=confusion_matrix_tic[0,0]\n",
    "fn=confusion_matrix_tic[1,0]\n",
    "fp=confusion_matrix_tic[0,1]\n",
    "tn=confusion_matrix_tic[1,1]\n",
    "\n",
    "tpr=round(tp/(tp+fn),2)\n",
    "fnr=round(fn/(tp+fn),2)\n",
    "fpr=round(fp/(fp+tn),2)\n",
    "tnr=round(tn/(fp+tn),2)\n",
    "print(\"TPR:\", tpr, \"FNR: \", fnr, \"FPR: \", fpr, \"TNR: \", tnr)\n",
    "\n",
    "\n",
    "confusion_matrix_german=confusion_mx(german, train_german, test_german, True, 1, 2)\n",
    "print(\"German:\")\n",
    "print(confusion_matrix_german)\n",
    "tp=confusion_matrix_german[0,0]\n",
    "fn=confusion_matrix_german[1,0]\n",
    "fp=confusion_matrix_german[0,1]\n",
    "tn=confusion_matrix_german[1,1]\n",
    "\n",
    "tpr=round(tp/(tp+fn),2)\n",
    "fnr=round(fn/(tp+fn),2)\n",
    "fpr=round(fp/(fp+tn),2)\n",
    "tnr=round(tn/(fp+tn),2)\n",
    "print(\"TPR:\", tpr, \"FNR: \", fnr, \"FPR: \", fpr, \"TNR: \", tnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
