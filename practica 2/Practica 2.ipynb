{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2.\n",
    "\n",
    "## Semana 1. Vecinos próximos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names:\n",
      "['Pregs', 'Plas', 'Pres', 'Skin', 'Test', 'Mass', 'Pedi', 'Age', 'Class']\n",
      "['Atributo1', 'Atributo2', 'Atributo3', 'Atributo4', 'Atributo5', 'Atributo6', 'Atributo7', 'Atributo8', 'Atributo9', 'Atributo10', 'Atributo11', 'Atributo12', 'Atributo13', 'Atributo14', 'Atributo15', 'Atributo16', 'Atributo17', 'Atributo18', 'Atributo19', 'Atributo20', 'Atributo21', 'Atributo22', 'Atributo23', 'Atributo24', 'Atributo25', 'Atributo26', 'Atributo27', 'Atributo28', 'Atributo29', 'Atributo30', 'Class']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mode \n",
    "\n",
    "filename=\"ConjuntosDatosP2/pima-indians-diabetes.data\"\n",
    "filename2=\"ConjuntosDatosP2/wdbc.data\"\n",
    "data = pd.read_csv(filename)\n",
    "data1=pd.read_csv(filename2)\n",
    "\n",
    "column_names=list(data.columns)\n",
    "column_names1=list(data1.columns)\n",
    "print(\"Column names:\")\n",
    "print(column_names)\n",
    "print(column_names1)\n",
    "class_name=\"Class\"\n",
    "\n",
    "class_list=data[class_name].values\n",
    "class_list1=data1[class_name].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mode \n",
    "class ClasificadorVecinosProximos:\n",
    "    def __init__(self,train_set, classes):\n",
    "        self.train=train_set.copy(deep=True)\n",
    "        self.classes=classes.values\n",
    "        self.class_names=np.unique(self.classes)\n",
    "        self.normalize_done=False\n",
    "        self.calcularMediasDesv()\n",
    "        \n",
    "    def calcularMediasDesv(self):\n",
    "        self.mean=self.train.mean(axis=0)\n",
    "        self.std=self.train.std(axis=0)\n",
    "    def normalizarDato(self, dato):\n",
    "        x=(dato-self.mean.values)/self.std.values\n",
    "        return(x)\n",
    "    def normalizarDatos(self):\n",
    "        if not self.normalize_done:\n",
    "            for att in self.train.columns:\n",
    "                self.train[att]=(self.train[att]-self.mean[att])/self.std[att]\n",
    "                self.normalize_done=True\n",
    "        return(self.train)\n",
    "\n",
    "    def euclidian_distance(self,dato):\n",
    "        i=0\n",
    "        square_dist=0\n",
    "        for att in self.train.columns:\n",
    "            square_dist=square_dist+(self.train[att].values-dato[i])**2\n",
    "            i+=1\n",
    "        square_dist=square_dist**0.5\n",
    "        return(square_dist)\n",
    "    def manhatten_distance(self,dato):\n",
    "        i=0\n",
    "        dist=0\n",
    "        for att in self.train.columns:\n",
    "            dist=dist+(self.train[att].values-dato[i])\n",
    "            i+=1\n",
    "        return(abs(dist))\n",
    "    def mahalanobis_distance(self,dato): \n",
    "        X=self.train.values\n",
    "        if self.normalize_done:\n",
    "            mu=np.zeros(len(dato))\n",
    "        else:\n",
    "            mu=self.mean\n",
    "        cov=np.cov(np.transpose(X-mu))\n",
    "        diff=dato - mu\n",
    "        inv=np.linalg.inv(cov)\n",
    "        left=np.dot(diff,inv)\n",
    "        dist=np.dot(left,np.transpose(diff))\n",
    "        return(dist)\n",
    "    def kneigbour_class(self, x, k, dist_type):\n",
    "        if dist_type==\"manhatten\":\n",
    "            dist=self.manhatten_distance(x)\n",
    "        elif dist_type==\"mahalanobis\":\n",
    "            dist=self.mahalanobis_distance(x)\n",
    "        else:\n",
    "            dist=self.euclidian_distance(x)\n",
    "        idx = np.argpartition(dist, k)\n",
    "        idx=idx[0:k]\n",
    "        return mode(self.classes[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import EstrategiaParticionado\n",
    "\n",
    "estrategia=EstrategiaParticionado.ValidacionSimple(0.3, 1)\n",
    "parts = estrategia.creaParticiones(data)\n",
    "test_row_id=parts[0].indicesTest\n",
    "train_row_id=parts[0].indicesTrain\n",
    "\n",
    "atributes=['Pregs', 'Plas', 'Pres', 'Skin', 'Test', 'Mass', 'Pedi', 'Age']\n",
    "\n",
    "#Selecting only columns with atributes required\n",
    "train=data.loc[train_row_id,atributes].copy()\n",
    "train_class_list=data.loc[train_row_id,class_name]\n",
    "\n",
    "\n",
    "test=data.loc[test_row_id,atributes]\n",
    "test_class_list=data.loc[test_row_id,class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "Dataset:  ConjuntosDatosP2/pima-indians-diabetes.data\n",
      "Atributos utilizados:  ['Pregs', 'Plas', 'Pres', 'Skin', 'Test', 'Mass', 'Pedi', 'Age']\n",
      "Datos normalizados, distancia euclidia\n",
      "K:  1 0.3\n",
      "K:  3 0.29\n",
      "K:  5 0.3\n",
      "K:  11 0.3\n",
      "K:  21 0.27\n",
      "Datos normalizados, distancia manhatten\n",
      "K:  1 0.38\n",
      "K:  3 0.33\n",
      "K:  5 0.34\n",
      "K:  11 0.32\n",
      "K:  21 0.3\n",
      "Datos sin normalizar, distancia euclidia\n",
      "K:  1 0.34\n",
      "K:  3 0.3\n",
      "K:  5 0.29\n",
      "K:  11 0.28\n",
      "K:  21 0.3\n",
      "Datos sin normalizar, distancia manhatten\n",
      "K:  1 0.41\n",
      "K:  3 0.39\n",
      "K:  5 0.39\n",
      "K:  11 0.36\n",
      "K:  21 0.34\n"
     ]
    }
   ],
   "source": [
    "def error(model, test, test_classes, normalize, k, distance):\n",
    "    err=0\n",
    "    for index, row in test.iterrows():\n",
    "        if normalize:\n",
    "            row=model.normalizarDato(row.values)\n",
    "        predicted=model.kneigbour_class(row, k, distance)\n",
    "        if predicted!=test_classes.loc[index]:\n",
    "            err+=1\n",
    "    return (round(err/test.shape[0],2))\n",
    "\n",
    "k_n=[1, 3, 5, 11, 21]\n",
    "\n",
    "\n",
    "mod_kn=ClasificadorVecinosProximos(train, train_class_list)\n",
    "norm_data=mod_kn.normalizarDatos()\n",
    "\n",
    "print(\"Error:\")\n",
    "print(\"Dataset: \", filename)\n",
    "print(\"Atributos utilizados: \", atributes)\n",
    "print(\"Datos normalizados, distancia euclidia\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod_kn, test, test_class_list, True, k, \"euclidian\"))\n",
    "print(\"Datos normalizados, distancia manhatten\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod_kn, test, test_class_list, True, k, \"manhatten\"))\n",
    "\n",
    "    \n",
    "  \n",
    "mod=ClasificadorVecinosProximos(train, train_class_list)  \n",
    "print(\"Datos sin normalizar, distancia euclidia\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod, test, test_class_list, False, k, \"euclidian\"))\n",
    "print(\"Datos sin normalizar, distancia manhatten\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod, test, test_class_list, False, k, \"manhatten\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "estrategia=EstrategiaParticionado.ValidacionSimple(0.3, 1)\n",
    "parts1 = estrategia.creaParticiones(data1)\n",
    "test_row_id1=parts1[0].indicesTest\n",
    "train_row_id1=parts1[0].indicesTrain\n",
    "\n",
    "atributes1=['Atributo1', 'Atributo2', 'Atributo3', 'Atributo4', 'Atributo5', 'Atributo6', 'Atributo7', 'Atributo8', 'Atributo9', 'Atributo10', 'Atributo11', 'Atributo12', 'Atributo13', 'Atributo14', 'Atributo15', 'Atributo16', 'Atributo17', 'Atributo18', 'Atributo19', 'Atributo20', 'Atributo21', 'Atributo22', 'Atributo23', 'Atributo24', 'Atributo25', 'Atributo26', 'Atributo27', 'Atributo28', 'Atributo29', 'Atributo30']\n",
    "\n",
    "#Selecting only columns with atributes required\n",
    "train1=data1.loc[train_row_id1,atributes1].copy()\n",
    "train_class_list1=data1.loc[train_row_id1,class_name]\n",
    "\n",
    "\n",
    "test1=data1.loc[test_row_id1,atributes1]\n",
    "test_class_list1=data1.loc[test_row_id1,class_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:\n",
      "Dataset:  ConjuntosDatosP2/wdbc.data\n",
      "Atributos utilizados:  ['Atributo1', 'Atributo2', 'Atributo3', 'Atributo4', 'Atributo5', 'Atributo6', 'Atributo7', 'Atributo8', 'Atributo9', 'Atributo10', 'Atributo11', 'Atributo12', 'Atributo13', 'Atributo14', 'Atributo15', 'Atributo16', 'Atributo17', 'Atributo18', 'Atributo19', 'Atributo20', 'Atributo21', 'Atributo22', 'Atributo23', 'Atributo24', 'Atributo25', 'Atributo26', 'Atributo27', 'Atributo28', 'Atributo29', 'Atributo30']\n",
      "Datos normalizados, distancia euclidia\n",
      "K:  1 0.03\n",
      "K:  3 0.02\n",
      "K:  5 0.03\n",
      "K:  11 0.02\n",
      "K:  21 0.03\n",
      "Datos normalizados, distancia manhatten\n",
      "K:  1 0.17\n",
      "K:  3 0.13\n",
      "K:  5 0.13\n",
      "K:  11 0.12\n",
      "K:  21 0.12\n",
      "Datos sin normalizar, distancia euclidia\n",
      "K:  1 0.09\n",
      "K:  3 0.06\n",
      "K:  5 0.06\n",
      "K:  11 0.09\n",
      "K:  21 0.07\n",
      "Datos sin normalizar, distancia manhatten\n",
      "K:  1 0.13\n",
      "K:  3 0.12\n",
      "K:  5 0.11\n",
      "K:  11 0.1\n",
      "K:  21 0.11\n"
     ]
    }
   ],
   "source": [
    "mod_kn1=ClasificadorVecinosProximos(train1, train_class_list1)\n",
    "norm_data1=mod_kn1.normalizarDatos()\n",
    "\n",
    "print(\"Error:\")\n",
    "print(\"Dataset: \", filename2)\n",
    "print(\"Atributos utilizados: \", atributes1)\n",
    "print(\"Datos normalizados, distancia euclidia\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod_kn1, test1, test_class_list1, True, k, \"euclidian\"))\n",
    "print(\"Datos normalizados, distancia manhatten\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod_kn1, test1, test_class_list1, True, k, \"manhatten\"))\n",
    "\n",
    "    \n",
    "mod1=ClasificadorVecinosProximos(train1, train_class_list1)  \n",
    "print(\"Datos sin normalizar, distancia euclidia\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod1, test1, test_class_list1, False, k, \"euclidian\"))\n",
    "print(\"Datos sin normalizar, distancia manhatten\")\n",
    "for k in k_n:\n",
    "    print(\"K: \", k, error(mod1, test1, test_class_list1, False, k, \"manhatten\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Error:\n",
      "ConjuntosDatosP2/pima-indians-diabetes.data error 0.22\n",
      "ConjuntosDatosP2/wdbc.data error 0.11\n"
     ]
    }
   ],
   "source": [
    "from Datos import *\n",
    "import Clasificador\n",
    "print(\"Naive Bayes Error:\")\n",
    "dataset_nb=Datos(filename)\n",
    "\n",
    "cl_nb=Clasificador.ClasificadorNaiveBayes(True)\n",
    "errores=cl_nb.validacion(estrategia,dataset_nb,cl_nb)\n",
    "print(filename, \"error\", round(errores[0],2))\n",
    "\n",
    "dataset_nb1=Datos(filename2)\n",
    "\n",
    "cl_nb1=Clasificador.ClasificadorNaiveBayes(True)\n",
    "errores1=cl_nb1.validacion(estrategia,dataset_nb1,cl_nb1)\n",
    "print(filename2, \"error\", round(errores1[0],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      "Dataset:  ConjuntosDatosP2/pima-indians-diabetes.data\n",
      "Datos normalizados, distancia euclidia, k = 3\n",
      "[[ 44.  24.]\n",
      " [ 42. 120.]]\n",
      "Datos sin normalizar, distancia euclidia, k = 3\n",
      "[[ 44.  27.]\n",
      " [ 42. 117.]]\n",
      "Dataset:  ConjuntosDatosP2/wdbc.data\n",
      "Datos normalizados, distancia euclidia, k = 3\n",
      "[[ 51.   1.]\n",
      " [  3. 116.]]\n",
      "Datos sin normalizar, distancia euclidia, k = 3\n",
      "[[ 49.   6.]\n",
      " [  5. 111.]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_mx_knn(model, test, test_classes, normalize, k, distance, positive, negative):\n",
    "    confusion_matrix=np.zeros([2,2])\n",
    "    for index, row in test.iterrows():\n",
    "        if normalize:\n",
    "            row=model.normalizarDato(row.values)\n",
    "        predicted=model.kneigbour_class(row, k, distance)\n",
    "        real=test_classes.loc[index]\n",
    "        if predicted==positive:\n",
    "            if real==positive:\n",
    "                confusion_matrix[0,0]+=1\n",
    "            else: \n",
    "                confusion_matrix[0,1]+=1\n",
    "        else:\n",
    "            if real==negative:\n",
    "                confusion_matrix[1,1]+=1\n",
    "            else: \n",
    "                confusion_matrix[1,0]+=1\n",
    "\n",
    "    return(confusion_matrix)\n",
    "\n",
    "k=3\n",
    "print(\"Matriz de confusión\")\n",
    "print(\"Dataset: \", filename)\n",
    "print(\"Datos normalizados, distancia euclidia, k =\", k)\n",
    "print(confusion_mx_knn(mod_kn, test, test_class_list, True, k, \"euclidian\", 1, 0))\n",
    "print(\"Datos sin normalizar, distancia euclidia, k =\", k)\n",
    "print(confusion_mx_knn(mod, test, test_class_list, False, k, \"euclidian\", 1, 0))\n",
    "\n",
    "print(\"Dataset: \", filename2)\n",
    "\n",
    "print(\"Datos normalizados, distancia euclidia, k =\",k)\n",
    "print(confusion_mx_knn(mod_kn1, test1, test_class_list1, True, k, \"euclidian\", \"M\", \"B\"))\n",
    "print(\"Datos sin normalizar, distancia euclidia, k =\", k)\n",
    "print(confusion_mx_knn(mod1, test1, test_class_list1, False, k, \"euclidian\", \"M\", \"B\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test fields for calculating mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.     35.      0.     33.6     0.627  50.   ]\n",
      " [  1.     85.     66.     29.      0.     26.6     0.351  31.   ]]\n",
      "[[2440.09184827 1497.91970962]\n",
      " [1497.91970962  993.49795012]]\n",
      "[[ 0.00550528 -0.00830044]\n",
      " [-0.00830044  0.0135213 ]]\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7320508075688772"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "arr1=test.loc[0].values\n",
    "arr2=train.loc[1].values\n",
    "m=np.matrix([arr1,arr2])\n",
    "print(m)\n",
    "Covariance = np.cov(m)\n",
    "print(Covariance)\n",
    "inv = np.linalg.inv(Covariance)\n",
    "print(inv)\n",
    "iv = [[1, 0.5, 0.5], [0.5, 1, 0.5], [0.5, 0.5, 1]]\n",
    "print(distance.mahalanobis([1, 0, 0], [0, 1, 0], iv))\n",
    "distance.mahalanobis([0, 2, 0], [0, 1, 0], iv)\n",
    "distance.mahalanobis([2, 0, 0], [0, 1, 0], iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.246646338576568\n"
     ]
    }
   ],
   "source": [
    "#create function to calculate Mahalanobis distance\n",
    "def mahalanobis(x, y, cov=None):\n",
    "    x_mean = np.mean(x)\n",
    "    Covariance = np.cov(np.transpose(y))\n",
    "    inv_covmat = np.linalg.inv(Covariance)\n",
    "    x_minus_mn = x - y_mean\n",
    "    D_square = np.dot(np.dot(x_minus_mn, inv_covmat), np.transpose(x_minus_mn))\n",
    "    return D_square\n",
    "\n",
    "#create new column in dataframe that contains Mahalanobis distance for each row\n",
    "dist = mahalanobis(test.loc[0], train)\n",
    "#display first five rows of dataframe\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dato' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-f5e6fc0502b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mmu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdato\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dato' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "iv = [[1, 0.5, 0.5], [0.5, 1, 0.5], [0.5, 0.5, 1]]\n",
    "print(distance.mahalanobis([1, 0, 0], [0, 1, 0], iv))\n",
    "distance.mahalanobis([0, 2, 0], [0, 1, 0], iv)\n",
    "distance.mahalanobis([2, 0, 0], [0, 1, 0], iv)\n",
    "\n",
    "X=norm_data[atributes].values\n",
    "mu=np.zeros(len(dato))\n",
    "print(mu)\n",
    "cov=np.dot(np.transpose(X),X)\n",
    "print(np.cov(np.transpose(X)))\n",
    "diff=dato - mu\n",
    "print(diff)\n",
    "inv=np.linalg.inv(cov)\n",
    "left=np.dot(np.transpose(diff),inv)\n",
    "dist=np.dot(left,diff)\n",
    "print(cov)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'column_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-106dac68df12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mexample\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'column_ids' is not defined"
     ]
    }
   ],
   "source": [
    "#for i in range(data.shape[0])\n",
    "\n",
    "matrix=np.matrix(norm_data)\n",
    "matrix=np.transpose(matrix)\n",
    "example=matrix[column_ids]\n",
    "\n",
    "\n",
    "X = np.stack((example, np.matrix(norm_point)), axis=0)\n",
    "print(X)\n",
    "cov=np.cov(X)\n",
    "print(cov)\n",
    "dist=(np.transpose(example - norm_point)*np.linalg.inv(cov)*(example - norm_point))**0.5\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semana 2. Regresión logistica\n",
    "ClasificadorRegresionLogistica\n",
    "\n",
    "\n",
    "Indians:\n",
    "\n",
    "preg = Number of times pregnant\n",
    "\n",
    "plas = Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "\n",
    "pres = Diastolic blood pressure (mm Hg)\n",
    "\n",
    "skin = Triceps skin fold thickness (mm)\n",
    "\n",
    "test = 2-Hour serum insulin (mu U/ml)\n",
    "\n",
    "mass = Body mass index (weight in kg/(height in m)^2)\n",
    "\n",
    "pedi = Diabetes pedigree function\n",
    "\n",
    "age = Age (years)\n",
    "\n",
    "class = Class variable (1:tested positive for diabetes, 0: tested negative for diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClasificadorRegresionLogistica:\n",
    "    def __init__(self,train_set, classes):\n",
    "        self.train_set=train_set\n",
    "        self.classes=classes\n",
    "        self.class_names=np.unique(self.classes)\n",
    "        self.class1=self.class_names[0]\n",
    "    def gradient_descent(self, rounds, eta):\n",
    "        self.w=np.zeros(self.train_set.shape[1]+1)+0.5\n",
    "        for r in range(rounds):\n",
    "            #for i in range(self.train_set.shape[0]):\n",
    "            for i, row in self.train_set.iterrows():\n",
    "                #x=np.array(self.train_set.iloc[i].values)\n",
    "                x=np.array(row.values)\n",
    "                x=np.insert(x,0,1)\n",
    "                x=np.transpose(x)\n",
    "                if self.classes[i]==self.class1:\n",
    "                    t=1\n",
    "                else:\n",
    "                    t=0\n",
    "                wx=np.dot(self.w,x)\n",
    "                self.w=self.w-eta*(1/(1+np.exp(-wx))-t)*x\n",
    "        return(self.w)\n",
    "    def classify(self,x):\n",
    "        x=np.array(x)\n",
    "        x=np.insert(x,0,1)\n",
    "        x=np.transpose(x)\n",
    "        wx=np.dot(self.w,x)\n",
    "        if wx>=0:\n",
    "            return self.class1\n",
    "        else:\n",
    "            return self.class_names[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.48462213 -3.2996518  -1.95167018  2.68811167  1.13201183 -2.27855089\n",
      "  3.45473797  0.36901397  1.59798852]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "atributes=[\"Pregs\",\"Plas\",\"Pres\",\"Skin\",\"Test\"]\n",
    "filter_zeros=[False, True, True, True, True]\n",
    "\n",
    "\n",
    "#This one is not active now, but the data should be filtered previously as it contains 0 fields (first dataset)\n",
    "#Filtering zero values\n",
    "for i in range(len(atributes)):\n",
    "    if filter_zeros[i]:\n",
    "        data_filtered=data.loc[data[atributes[i]]!=0]\n",
    "train_set=data_filtered[atributes]\n",
    "classes=data_filtered[class_name].values\n",
    "\n",
    "\n",
    "model_rg=ClasificadorRegresionLogistica(train,train_class_list)\n",
    "w=model_rg.gradient_descent(10, 0.01)\n",
    "print(w)\n",
    "#print(test.iloc[1].values)\n",
    "x=[6,148,72,35,0,33.6,0.627,50]\n",
    "print(model_rg.classify(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error regresión logistica, dataset ConjuntosDatosP2/pima-indians-diabetes.data\n",
      "0.43\n",
      "Error regresión logistica, dataset ConjuntosDatosP2/wdbc.data\n",
      "0.09\n"
     ]
    }
   ],
   "source": [
    "def error_rg(model, test, test_classes):\n",
    "    err=0\n",
    "    for index, row in test.iterrows():\n",
    "        predicted=model.classify(row)\n",
    "        if predicted!=test_classes.loc[index]:\n",
    "            err+=1\n",
    "    return (round(err/test.shape[0],2))\n",
    "\n",
    "model_rg=ClasificadorRegresionLogistica(train,train_class_list)\n",
    "w=model_rg.gradient_descent(10, 0.01)\n",
    "print(\"Error regresión logistica, dataset\", filename)\n",
    "print(error_rg(model_rg, test, test_class_list))\n",
    "model_rg1=ClasificadorRegresionLogistica(train1,train_class_list1)\n",
    "w=model_rg1.gradient_descent(10, 0.01)\n",
    "print(\"Error regresión logistica, dataset\", filename2)\n",
    "print(error_rg(model_rg1, test1, test_class_list1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
